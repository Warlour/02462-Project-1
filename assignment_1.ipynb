{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File name friendly\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "def slugify(value, allow_unicode=False):\n",
    "    \"\"\"\n",
    "    Taken from https://github.com/django/django/blob/master/django/utils/text.py\n",
    "    Convert to ASCII if 'allow_unicode' is False. Convert spaces or repeated\n",
    "    dashes to single dashes. Remove characters that aren't alphanumerics,\n",
    "    underscores, or hyphens. Convert to lowercase. Also strip leading and\n",
    "    trailing whitespace, dashes, and underscores.\n",
    "    \"\"\"\n",
    "    value = str(value)\n",
    "    if allow_unicode:\n",
    "        value = unicodedata.normalize('NFKC', value)\n",
    "    else:\n",
    "        value = unicodedata.normalize('NFKD', value).encode('ascii', 'ignore').decode('ascii')\n",
    "    value = re.sub(r'[^\\w\\s-]', '', value.lower())\n",
    "    return re.sub(r'[-\\s]+', '-', value).strip('-_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 - CNN's and VGG16\n",
    "\n",
    "*In this assingment, you will further familiraize yourself with CNN's and how to implement them. For this particular example, we will ask you to implement the layer structure of VGG16, an old but fairly effective and simple CNN structure.*\n",
    "\n",
    "*Keep in mind, that while VGG16 and other CNN's you have implemented so far, only incoporate convolutions and pooling layers, many state-of-the-art models  use a variety of other techniques, such as skip connections (CITATION NEEDED), or self-attention (CITATION NEEDED) to get better results.*\n",
    "\n",
    "*As you write code for this assignment, try to keep in mind to write good code. That might sound vague, but just imagine that some other poor sod will have to read your code at some point, and easily readable, understandable code, will go a long way to making their life easier. However, this is not a coding course, so the main focus should of course be on the exercises themselves.*\n",
    "\n",
    "**Keep in mind, this assignment does not count towards your final grade in the course. When any of the exercises mention 'grading', it refers to commenting and correcting answers, not necessarily giving you a score which will reflect in your grade, so dw :)**\n",
    "\n",
    "\n",
    "**Hand-in date is 8/10 at the latest if you want to recieve feedback!!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Boilerplate start - you can mostly ignore this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import PIL\n",
    "from torch import nn\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torchsummary import summary\n",
    "\n",
    "import utils\n",
    "from datetime import datetime\n",
    "\n",
    "# Check if you have cuda available, and use if you do\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Set a random seed for everything important\n",
    "def seed_everything(seed: int):\n",
    "    import random, os\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    \n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set a seed with a random integer, in this case, I choose my verymost favourite sequence of numbers\n",
    "seed_everything(sum([115, 107, 105, 98, 105, 100, 105, 32, 116, 111, 105, 108, 101, 116]))\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify dataset you wanna use\n",
    "def get_dataset(dataset_name, validation_size=0.1, transform=None, v=True):\n",
    "\n",
    "    if transform is None:\n",
    "        transform = ToTensor()\n",
    "\n",
    "    if dataset_name == 'cifar10':\n",
    "        train_set = datasets.CIFAR10(root='./data', train=True, download=True, transform=ToTensor())\n",
    "        test_set = datasets.CIFAR10(root='./data', train=False, download=True, transform=ToTensor())\n",
    "\n",
    "        # Purely for our convenience - Mapping from cifar labels to human readable classes\n",
    "        cifar10_classes = {\n",
    "            0: 'airplane',\n",
    "            1: 'automobile',\n",
    "            2: 'bird',\n",
    "            3: 'cat',\n",
    "            4: 'deer',\n",
    "            5: 'dog',\n",
    "            6: 'frog',\n",
    "            7: 'horse',\n",
    "            8: 'ship',\n",
    "            9: 'truck'\n",
    "        }\n",
    "\n",
    "    elif dataset_name == 'mnist':\n",
    "        train_set = datasets.MNIST(root='./data', train=True, download=True, transform=ToTensor())\n",
    "        test_set = datasets.MNIST(root='./data', train=False, download=True, transform=ToTensor())\n",
    "\n",
    "    elif dataset_name == 'imagenette':\n",
    "        download = not os.path.exists('./data/imagenette2')\n",
    "\n",
    "        # Specific transform in the case we use imagenette\n",
    "        imagenette_transform = transforms.Compose([\n",
    "            transforms.Resize(256),        # Resize to 256x256\n",
    "            transforms.RandomCrop(224),    # Crop the center to 224x224\n",
    "            transforms.ToTensor(),         # Convert to tensor\n",
    "            transforms.Normalize(mean=[0.4650, 0.4553, 0.4258], std=[0.2439, 0.2375, 0.2457]) # Normalize each image, numbers because of function courtesy of chatgpt\n",
    "        ])\n",
    "        train_set = datasets.Imagenette(root='./data', split='train', download=download, size='full', transform=imagenette_transform)\n",
    "        test_set = datasets.Imagenette(root='./data', split='val', download=False, size='full', transform=imagenette_transform)\n",
    "    \n",
    "    # If we want a validation set of a given size, take it from train set\n",
    "    if validation_size is not None:\n",
    "        # These will both be of the torch.utils.data.Subset type (not the Dataset type), and are basically just mappings of indices\n",
    "        # This does not matter when we make Dataloaders of them, however\n",
    "        if dataset_name != 'imagenette':\n",
    "            train_set, validation_set = torch.utils.data.random_split(train_set, [1-validation_size, validation_size])\n",
    "\n",
    "        # In the case of imagenette, the 'test set' is already a pretty big validation set, so we'll use that to create the test set instead\n",
    "        else:\n",
    "            validation_set, test_set = torch.utils.data.random_split(test_set, [validation_size, 1-validation_size])\n",
    "\n",
    "    if v:\n",
    "        print(f\"There are {len(train_set)} examples in the training set\")\n",
    "        print(f\"There are {len(test_set)} examples in the test set \\n\")\n",
    "\n",
    "        print(f\"Image shape is: {train_set[0][0].shape}, label example is {train_set[0][1]}\")\n",
    "\n",
    "    return train_set, validation_set, test_set\n",
    "\n",
    "# collate function just to cast to device, same as in week_3 exercises\n",
    "def collate_fn(batch):\n",
    "    return tuple(x_.to(device) for x_ in default_collate(batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theoretical questions\n",
    "\n",
    "*These questions are meant to test your general knowledge of CNN's, feel free to contact or write the TA's if you have any questions about any of them*\n",
    "\n",
    "### Exercise 1.1\n",
    "\n",
    "**1. What is the reason we add MaxPooling or AveragePooling in CNN's?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pooling reduces the spatial dimensions (width and height) of the input feature maps, which leads to fewer parameters and less computational overhead in the network. This reduction helps in controlling overfitting and makes the model more efficient by compressing the representation.\n",
    "\n",
    "Max Pooling: Selects the maximum value from each patch of the input feature map. This helps in highlighting the most important (strongest) features in the feature map.\n",
    "\n",
    "Average Pooling: Computes the average value of each patch in the input feature map. This approach smooths the output and captures the overall average activation in each region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Pooling](Images/pooling.png \"Pooling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\\*2. Say a network comes with a list of class probabilities:** $\\hat{p}_1, \\hat{p}_2, \\dots \\hat{p}_N$ **when is the cross-entropy in regards to the *true* class probabilities:** $p_1, p_2, \\dots p_N$ **maximized?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-entropy is a loss function commonly used in classification tasks to measure the difference between two probability distributions: the true distribution and the predicted distribution. The goal of the learning algorithm is to minimize this difference, making the predicted probabilities as close as possible to the true ones.\n",
    "\n",
    "In the worst-case scenario, the maximum cross-entropy occurs when the predicted distribution is as far away as possible from the true distribution. \n",
    "\n",
    "Mathematically, the cross-entropy between a true probability distribution $p$ and a predicted distribution  $\\hat{p}$  is maximized when the true class has  $p_{i} = 1$ (i.e., the correct class), and the predicted probability for that class $\\hat{p}_{i} = 0$. That is, when the network is completely confident in the wrong class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. In the [VGG paper](https://arxiv.org/pdf/1409.1556), last paragraph of 'training', page 4, they mention images being randomly cropped after being rescaled. Why do you think they crop images only *after* rescaling them?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rescaling the images to a range of sizes (in the second approach) ensures that the network can recognize objects at multiple scales. Since objects can appear in different sizes in real-world images, this allows the model to be more robust and adaptable to variations in object size.\n",
    "\n",
    "After rescaling, random cropping is applied to get the final input size (e.g., 224x224). This firstly serves to prevent  distortion of the objects within the image, and secondly, random cropping introduces variability into the training data, effectively increasing the diversity of the training set. This prevents the model from overfitting to specific object locations within the images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. After this, they mention \"further augmenting the dataset\" by random horizontal flipping and random RGB color shift. Why do you think they do this?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random horizontal flipping helps the model generalize to images with a wider range of orientations, and similarly the RGB color shift trains the model to adapt to different lighting conditions and colors, forcing the weights to prioritize more robust classification features, like shape, texture, and structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\\*5. Why do you think they do not randomly translate images? (Translate being to move images left, right, up, down)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random translations are not very effective in this context because convolutional layers inherently provide a degree of translation invariance. This is due to the nature of convolution, where the same kernel slides across the image and detects patterns regardless of their exact location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Which of the following classification tasks do you think is more difficult for a machine learning model, and why?**\n",
    "\n",
    "- **Telling German Shepherds (Schæferhunde) from Labradors**\n",
    "- **Telling dogs from cats**\n",
    "- **Telling horses from cars from totem poles from chainsaws**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We believe telling German shepherds from Labradors is the most difficult tasks among the above, because the features for both classes will be more similar and thus have a greater overlap, making it harder to correctly classify. E.g. the shape of German Shepherds is more similar to Labradors than Dogs are to Cats.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. In real life, you often find that neural networks aren't used \"for everything\", older and often more simple models like random forest and linear regression still dominate a lot of fields.**\n",
    "\n",
    "- **Reason a bit about why this is the case**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In terms of reliability:** Using NN (Neural Network) depends on specific settings, meaning a large amount of data is required to build an effective model. In contrast, simpler models can capture trends without needing the complexity of a NN. Interestingly, with NN, you can completely misinterpret the underlying problem but still create an effective model with a large amount of data. Additionally, understanding a problem will always make it easier to implement a much simpler solution. Neural networks are also very sensitive to noise, whereas simpler models are typically more robust against such noise. \n",
    "\n",
    "**Resources:** When the large amount of data isn't available, you have to gather it yourself, this can be very pricy, whereas it's easier to build a simple model. Training a good model can also require significant computational resources, but when you can't borrow such powerful machinery, the cost is very high for something you probably only use for one project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\\*8. When we sample from our dataloader, we sample in batches, why is this? What would be the alternatives to sampling in batches, and what impact would that have?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training a neural network, we pass the data through the network multiple times to adjust the weights, and a full pass through the training data is called an epoch. However, when the dataset is too large to fit into memory at once, we divide the data into smaller subsets called batches. The batch size represents the number of training samples in each subset (or batch) processed at a time.\n",
    "\n",
    "If we don’t use batches, there are two alternatives: processing the data either one sample at a time (Stochastic Gradient Descent) or using the entire dataset in a single update (Full Batch Gradient Descent). Here’s what would happen in each case:\n",
    "\n",
    "**Stochastic Gradient Descent:**\n",
    "\n",
    "If we pass one sample at a time through the network (i.e., use a batch size of 1), the model’s weights will be updated after each individual data point. This leads to highly variable gradient updates, which may cause the model’s loss to fluctuate greatly and converge more slowly.\n",
    "\n",
    "**Full Batch Gradient Descent:**\n",
    "\n",
    "If we try to feed the entire dataset into the network at once, especially with very large datasets, the large context window would likely use in excess of the available memory (either CPU or GPU). This could cause out-of-memory errors or severely limit the size of datasets we can use for training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9. The VGG16-D conv layers all use the same kernel size. Come up with reasons for why you would use bigger/smaller kernel sizes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The selection of a kernel size depends on the goal in mind. In example, if you want to create an object detection model and you want to capture a car in whole as an object, you would generally use a larger kernel size, but if you wanted to capture the components of the car, you would use a smaller kernel size to e.g. capture the mirror or the wheels' features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\\*10. The \"new kid on the block' (relatively speaking) in NLP (Natural Language processing), is self-attention. Basically this is letting each word/token relate to each other word/token by a specific 'attention' value, vaguely showing how much they relate to one another.**\n",
    "\n",
    "- **Would there be any problems in doing this for image processing by simply letting each pixel relate to each other pixel, so we can get spatial information that way instead?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When applying self-attention in image processing, this would entail computing an attention score for each pair of pixels, enabling the model to capture dependencies, regardless of their distance in the image.\n",
    "\n",
    "The main challenge of applying self-attention to every pixel in an image is the quadratic complexity in terms of both computation and memory. Self-attention requires computing pairwise relationships between all pixels, meaning that for an image with $N$ pixels, you would need to compute $N \\times N$ attention scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boilerplate end - Your implementation work begins here:\n",
    "\n",
    "*Below, you are given a working example of a CNN, not much different from the one in the exercises of week 3. Your job is to complete the implementation questions below. *\n",
    "\n",
    "*You do not need to do all the exercises below, or even do them in order, we will obviously only grade the ones you have done, however. Please just mark completed exercises with an X as shown below, so we will know what to look for when grading your assignment. You can add as much text below each question as you want to either argue for your choice of implementation, discuss your results, or ask us questions, we will consider this when grading the assignment.*\n",
    "\n",
    "**X 0. This marks a question which has been completed**\n",
    "\n",
    "*For your convenience, we reccommend implementing two models: One bigger for the VGG16-D exercises, meant to be used only with images from the Imagenette dataset, and one smaller, which can also take the other datasets. The model already implemented below should fill the role of the latter.*\n",
    "\n",
    "*Finally, if you're not able to train the VGG16-D model because it is too big, you can also load the weights of the model using the funciton implemented for exactly that. We do, however, reccommend training it from scratch yourself, if possible.*\n",
    "\n",
    "______________________________________________________________________________________________\n",
    "\n",
    "#### **Question from us: We are unsure what is meant by \"You do not need to do all the exercises below\", is this not relevant for the exam?**\n",
    "\n",
    "\n",
    "X **1. Implement the layer structure of VGG16-D by following either this [Medium article](https://medium.com/@mygreatlearning/everything-you-need-to-know-about-vgg16-7315defb5918) (fairly easy), or the [official paper](https://arxiv.org/pdf/1409.1556) (slightly harder) (Note: This layer structure is meant to be used with 224x224 sized images, only the  imagenette dataset in this notebook has this)**\n",
    "\n",
    "X ****2. Figure out, and implement the type, and exact settings of the optimizer the original VGG16-D implementation used**  \n",
    "- Batch size: $256$\n",
    "- **Momentum: $0.9$**\n",
    "- **Weight Decay, $L_2$ Penalty Multiplier: $5\\times 10^{-4}$**\n",
    "- Dropout ratio: $0.5$\n",
    "- **Learning rate: $10^{-2}$, but decreased by a factor of 10 everytime the validation set accuracy stopped improving (done 3 times in the original VGG16).**\n",
    "- Epochs: 74 (We set ours to 20)  \n",
    "\n",
    "\n",
    "We did this by using SGD, but our accuracy was a terrible 10%, so we lowered our Batch size to 64 (mostly due to memory issues - even on HPC with 40 GB VRAM, but this could be due to others using parts of the GPU). We also changed back to Adam optimizer, since this would save us time from manually adjusting momentum and weight decay. Lastly, we decreased the learning rate to $10^{-4}$, and while we tested with the scheduler: \"ReduceLROnPlateau\", meant to mimic the learning rate factor decrease, we didn't see any improvement in accuracy.\n",
    "\n",
    "![Optimizer settings](models/settings.png \"Optimizer settings\")\n",
    "\n",
    "As seen on the plot, the amount of accuracy data points is significantly reduced with their settings, due to the high batch size of 256 yielding around 4 data points per epoch. However, if we wanted more datapoints, we could use the original dataset \"ImageNet\", which contains a 100 times more classes, with around 1000 images per class (like Imagenette).\n",
    "\n",
    "X **3.** \n",
    " - X **Can you make the VGG16-D model overfit to the imagenette dataset? If not, what about another dataset?**\n",
    "    - Remove dropout layers\n",
    "    - Increase amount of epochs with a low batch size.  \n",
    "\n",
    "    As seen on our previous picture, using their optimizer settings, the model was not learning from the training, but after switching to our own settings we see the accuracy steadily increase in proportion to the training epochs. However after around 10 epochs we begin to see an increasing difference in training and testing accuracy, which implies that the model is overfitting to the training data and has a hard time generalizing to new data.\n",
    "\n",
    " - X **Can you change the amount of dropout to increase or decrease the rate of overfitting?**\n",
    "    - Yes, if you increase dropout layers you decrease overfitting, by minimizing the amount of weight that's put on to a single neuron, by randomly dropping (set to zero) a certain percentage of neurons in a layer during each training pass, forcing the network to learn more robust features and preventing it from relying too heavily on specific neurons or paths.\n",
    "\n",
    "\n",
    "- **Can you make the smaller model overfit to any of its datasets? Is it harder or easier? Explain your answer**\n",
    "\n",
    "X **4. Try to improve the test accuracy of either of your models by changing some of they hyperparameters. To make it easier, try to keep detailed results of your experimental setups and your preliminary results. Argue for your changes. Examples of possible changes are shown below:**:\n",
    "- **Add more/fewer kernels**\n",
    "- **Add more/less dropout**\n",
    "- **Add [BatchNorm](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html)**\n",
    "- **Change the initial transform when loading the data to make larger/smaller images**\n",
    "\n",
    "Using our previous settings, training the model for around 75 epochs, we saw a good amount of overfitting after about 15 epochs, with the models testing accuracy converging at around that same point (15 epochs). \n",
    "\n",
    "![Converge](models/2024-10-07_21-34-31.png \"Converge\")\n",
    "\n",
    "To combat the issue of early converging and overfitting, we introduced early stopping and added batch normalization after each convolutional layer to improve generalization. As a result, we got the highest test accuracy yet, although, the accuracy was relatively unstable. The following image displays the training accuracy vs test accuracy, where we can observe that the model seems like it hasn't converged yet, meaning a higher accuracy is still possible. The overfitting was also decreased a little, meaning the batch normalization has helped a little in generalizing the data, but increased \"instability\".  \n",
    "\n",
    "![No converge](models/2024-10-08_13-35-44.png \"No converge\")\n",
    "\n",
    "\n",
    "X **5. Change the model (or any other code in the whole script) to make either training, inference, or both, as quick as possible, while still keeping a reasonable test accuracy. What did you do to achieve this?**\n",
    "\n",
    "- The model using batch size 64 and 20 epochs managed to get a 74% accuracy, while being computationally light in both training and inference. Notably we also ran the model using the same hyperparameters but with a batch size of 128, and here we got a 72% accuracy after 20 epochs, which implies no significant difference. Therefor, we found the model with smaller batch size preferable, since it runs faster.\n",
    "\n",
    "\n",
    "\n",
    "X **6. During evaluation, extract some images which were not correctly classified, plot these images and reason about what made them hard-to-classify**\n",
    "\n",
    "In image 0 we again see that the image has been cropped. The image belongs to the parachute class, however the cropped version doesn't contain the parachute, thus making it hard to make the right classification. Also the image contains a huge shark which becomes very obstructive for the classification, since it takes up many more pixel than the man with the parachute. Additionally the dimensions of the man seem out of proportion (his head is enlarged).\n",
    "\n",
    "In image 1 we see a man holding a chainsaw, which belongs to the chainsaw class. However the image has also been cropped such that the chainsaw is barely visible, making it much harder to classify. It seems feasible that a larger model would still be able to classify it based on other features such as the man wearing a helmet, but it seems that our model put particular emphasis on the chainsaw itself, since this classification failed.\n",
    "\n",
    "In image 7, we have a different issue where there image is zoomed in, hiding the contextual information such as a man holding the chainsaw, which also resulted in a misclassification. Additionally the chainsaw is very blurry and only the blade is visible.\n",
    "\n",
    "\n",
    "![Incorrect images](Images/incorrect_images_2024-10-07_21-34-31.png \"Incorrect images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 9469 examples in the training set\n",
      "There are 3532 examples in the test set \n",
      "\n",
      "Image shape is: torch.Size([3, 224, 224]), label example is 0\n"
     ]
    }
   ],
   "source": [
    "# Get data\n",
    "train_set, validation_set, test_set = get_dataset('imagenette', validation_size=0.1)\n",
    "\n",
    "# Make dataloaders\n",
    "batch_size=64 # Dramatically increases training time\n",
    "train_dataloader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "validation_dataloader = torch.utils.data.DataLoader(validation_set, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16D(torch.nn.Module):\n",
    "    def __init__(self, num_classes, in_channels=3, features_fore_linear=512*7*7, dataset=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Helper hyperparameters to keep track of VGG16 architecture\n",
    "        conv_stride = 1\n",
    "        pool_stride = 2\n",
    "        conv_kernel = 3\n",
    "        pool_kernel = 2\n",
    "        dropout_probs = 0.5\n",
    "        optim_momentum = 0.9\n",
    "        weight_decay = 5*10**(-4)\n",
    "        learning_rate = 0.0001\n",
    "\n",
    "        # Define features and classifier each individually, this is how the VGG16-D model is originally defined\n",
    "        self.features = torch.nn.Sequential(\n",
    "            # Conv Block 1\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=64, kernel_size=conv_kernel, padding=1, stride=conv_stride),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=conv_kernel, padding=1, stride=conv_stride),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=pool_kernel, stride=pool_stride), # Image size: 224 -> 112\n",
    "\n",
    "            # Conv Block 2\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=conv_kernel, padding=1, stride=conv_stride),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=conv_kernel, padding=1, stride=conv_stride),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=pool_kernel, stride=pool_stride), # Image size: 112 -> 56\n",
    "\n",
    "            # Conv Block 3\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=conv_kernel, padding=1, stride=conv_stride),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=conv_kernel, padding=1, stride=conv_stride),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=conv_kernel, padding=1, stride=conv_stride),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=pool_kernel, stride=pool_stride), # Image size: 56 -> 28\n",
    "\n",
    "            # Conv Block 4\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=conv_kernel, padding=1, stride=conv_stride),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=conv_kernel, padding=1, stride=conv_stride),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=conv_kernel, padding=1, stride=conv_stride),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=pool_kernel, stride=pool_stride), # Image size: 28 -> 14\n",
    "\n",
    "            # Conv Block 5\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=conv_kernel, padding=1, stride=conv_stride),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=conv_kernel, padding=1, stride=conv_stride),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=conv_kernel, padding=1, stride=conv_stride),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=pool_kernel, stride=pool_stride), # Image size: 14 -> 7\n",
    "            nn.Flatten(),\n",
    "\n",
    "        ).to(device)\n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            # FCL Block 6\n",
    "            nn.Linear(in_features=features_fore_linear, out_features=4096), # The same as dense layer\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_probs),\n",
    "            nn.Linear(in_features=4096, out_features=4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_probs),\n",
    "            nn.Linear(in_features=4096, out_features=num_classes),\n",
    "        ).to(device)\n",
    "        \n",
    "        # In the paper, they mention updating towards the 'multinomial logistic regression objective'\n",
    "        # As can be read in Bishop p. 159, taking the logarithm of this equates to the cross-entropy loss function.\n",
    "        # Softmax\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Optimizer - For now just set to Adam to test the implementation\n",
    "        self.optim = torch.optim.Adam(list(self.features.parameters()) + list(self.classifier.parameters()), lr=learning_rate)\n",
    "        # self.optim = torch.optim.SGD(list(self.features.parameters()) + list(self.classifier.parameters()), lr=learning_rate, momentum=optim_momentum, weight_decay=weight_decay)\n",
    "\n",
    "        # Learning rate scheduler for improved convergence\n",
    "        # self.scheduler = torch.optim.lr_scheduler.StepLR(self.optim, step_size=10, gamma=0.1)\n",
    "        # Use a scheduler like VGG16\n",
    "        # self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optim, mode='max', factor=0.1, patience=5, verbose=True)\n",
    "\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.classifier(self.features(x))\n",
    "\n",
    "    def train_model(self, train_dataloader, epochs=1, val_dataloader=None, early_stopping_patience=5):\n",
    "        \n",
    "        # Call .train() on self to turn on dropout\n",
    "        self.train()\n",
    "\n",
    "        # To hold accuracy during training and testing\n",
    "        train_accs = []\n",
    "        test_accs = []\n",
    "\n",
    "        # Early stopping var\n",
    "        best_acc = 0\n",
    "        patience_counter = 0\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            epoch_acc = 0\n",
    "\n",
    "            for inputs, targets in tqdm(train_dataloader):\n",
    "                logits = self(inputs)\n",
    "                loss = self.criterion(logits, targets)\n",
    "                loss.backward()\n",
    "\n",
    "                self.optim.step()\n",
    "                self.optim.zero_grad(set_to_none=True)\n",
    "\n",
    "                # Keep track of training accuracy\n",
    "                epoch_acc += (torch.argmax(logits, dim=1) == targets).sum().item()\n",
    "            train_accs.append(epoch_acc / len(train_dataloader.dataset))\n",
    "\n",
    "            # If val_dataloader, evaluate after each epoch\n",
    "            if val_dataloader is not None:\n",
    "                # Turn off dropout for testing\n",
    "                self.eval()\n",
    "                acc = self.eval_model(val_dataloader)\n",
    "                test_accs.append(acc)\n",
    "                print(f\"Epoch {epoch} validation accuracy: {acc}\")\n",
    "\n",
    "                if acc > best_acc:\n",
    "                    best_acc = acc\n",
    "                    patience_counter = 0\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                \n",
    "                if patience_counter >= early_stopping_patience:\n",
    "                    print(\"Early stopping triggered\")\n",
    "                    break\n",
    "\n",
    "                # turn on dropout after being done\n",
    "                self.train()\n",
    "        \n",
    "        return train_accs, test_accs\n",
    "\n",
    "    def eval_model(self, test_dataloader):\n",
    "        \n",
    "        self.eval()\n",
    "        total_acc = 0\n",
    "\n",
    "        for input_batch, label_batch in test_dataloader:\n",
    "            logits = self(input_batch)\n",
    "\n",
    "            total_acc += (torch.argmax(logits, dim=1) == label_batch).sum().item()\n",
    "\n",
    "        total_acc = total_acc / len(test_dataloader.dataset)\n",
    "\n",
    "        return total_acc\n",
    "\n",
    "    def predict(self, img_path):\n",
    "        img = PIL.Image.open(img_path)\n",
    "        img = self.dataset.dataset.transform(img)\n",
    "        classification = torch.argmax(self(img.unsqueeze(dim=0)), dim=1)\n",
    "        return img, classification\n",
    "    \n",
    "    def predict_random(self, num_predictions=16):\n",
    "        \"\"\"\n",
    "        Plot random images from own given dataset\n",
    "        \"\"\"\n",
    "        random_indices = np.random.choice(len(self.dataset)-1, num_predictions, replace=False)\n",
    "        classifications = []\n",
    "        labels = []\n",
    "        images = []\n",
    "        for idx in random_indices:\n",
    "            img, label = self.dataset.__getitem__(idx)\n",
    "\n",
    "            classification = torch.argmax(self(img.unsqueeze(dim=0)), dim=1)\n",
    "\n",
    "            classifications.append(classification)\n",
    "            labels.append(label)\n",
    "            images.append(img)\n",
    "\n",
    "        return classifications, labels, images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16S(torch.nn.Module):\n",
    "    def __init__(self, num_classes, in_channels=1, features_fore_linear=512*7*7, dataset=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Helper hyperparameters to keep track of VGG16 architecture\n",
    "        conv_stride = ...\n",
    "        pool_stride = ...\n",
    "        conv_kernel = ...\n",
    "        pool_kernel = ...\n",
    "        dropout_probs = ...\n",
    "        optim_momentum = ...\n",
    "        weight_decay = ...\n",
    "        learning_rate = ...\n",
    "\n",
    "        # Define features and classifier each individually, this is how the VGG16-D model is originally defined\n",
    "        self.features = torch.nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=32, kernel_size=3, padding=1), # dim = in\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "        ).to(device)\n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            nn.Linear(in_features=features_fore_linear, out_features=600),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=600, out_features=120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=120, out_features=num_classes)\n",
    "        ).to(device)\n",
    "        \n",
    "        # In the paper, they mention updating towards the 'multinomial logistic regression objective'\n",
    "        # As can be read in Bishop p. 159, taking the logarithm of this equates to the cross-entropy loss function.\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Optimizer - For now just set to Adam to test the implementation\n",
    "        self.optim = torch.optim.Adam(list(self.features.parameters()) + list(self.classifier.parameters()), lr=0.001)\n",
    "        # self.optim = torch.optim.SGD(list(self.features.parameters()) + list(self.classifier.parameters()), lr=learning_rate, momentum=optim_momentum, weight_decay=weight_decay)\n",
    "\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.classifier(self.features(x))\n",
    "\n",
    "    def train_model(self, train_dataloader, epochs=1, val_dataloader=None):\n",
    "        \n",
    "        # Call .train() on self to turn on dropout\n",
    "        self.train()\n",
    "\n",
    "        # To hold accuracy during training and testing\n",
    "        train_accs = []\n",
    "        test_accs = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            epoch_acc = 0\n",
    "\n",
    "            for inputs, targets in tqdm(train_dataloader):\n",
    "                logits = self(inputs)\n",
    "                loss = self.criterion(logits, targets)\n",
    "                loss.backward()\n",
    "\n",
    "                self.optim.step()\n",
    "                self.optim.zero_grad()\n",
    "\n",
    "                # Keep track of training accuracy\n",
    "                epoch_acc += (torch.argmax(logits, dim=1) == targets).sum().item()\n",
    "            train_accs.append(epoch_acc / len(train_dataloader.dataset))\n",
    "\n",
    "            # If val_dataloader, evaluate after each epoch\n",
    "            if val_dataloader is not None:\n",
    "                # Turn off dropout for testing\n",
    "                self.eval()\n",
    "                acc = self.eval_model(val_dataloader)\n",
    "                test_accs.append(acc)\n",
    "                print(f\"Epoch {epoch} validation accuracy: {acc}\")\n",
    "                # turn on dropout after being done\n",
    "                self.train()\n",
    "        \n",
    "        return train_accs, test_accs\n",
    "\n",
    "    def eval_model(self, test_dataloader):\n",
    "        \n",
    "        self.eval()\n",
    "        total_acc = 0\n",
    "\n",
    "        for input_batch, label_batch in test_dataloader:\n",
    "            logits = self(input_batch)\n",
    "\n",
    "            total_acc += (torch.argmax(logits, dim=1) == label_batch).sum().item()\n",
    "\n",
    "        total_acc = total_acc / len(test_dataloader.dataset)\n",
    "\n",
    "        return total_acc\n",
    "\n",
    "    def predict(self, img_path):\n",
    "        img = PIL.Image.open(img_path)\n",
    "        img = self.dataset.dataset.transform(img)\n",
    "        classification = torch.argmax(self(img.unsqueeze(dim=0)), dim=1)\n",
    "        return img, classification\n",
    "    \n",
    "    def predict_random(self, num_predictions=16):\n",
    "        \"\"\"\n",
    "        Plot random images from own given dataset\n",
    "        \"\"\"\n",
    "        random_indices = np.random.choice(len(self.dataset)-1, num_predictions, replace=False)\n",
    "        classifications = []\n",
    "        labels = []\n",
    "        images = []\n",
    "        for idx in random_indices:\n",
    "            img, label = self.dataset.__getitem__(idx)\n",
    "\n",
    "            classification = torch.argmax(self(img.unsqueeze(dim=0)), dim=1)\n",
    "\n",
    "            classifications.append(classification)\n",
    "            labels.append(label)\n",
    "            images.append(img)\n",
    "\n",
    "        return classifications, labels, images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-trained model\n",
    "def get_vgg_weights(model):\n",
    "    \"\"\"\n",
    "    Loads VGG16-D weights for the classifier to an already existing model\n",
    "    Also sets training to only the classifier\n",
    "    \"\"\"\n",
    "    # Load the complete VGG16 model\n",
    "    temp = torchvision.models.vgg16(weights='DEFAULT')\n",
    "\n",
    "    # Get its state dict\n",
    "    state_dict = temp.state_dict()\n",
    "\n",
    "    # Change the last layer to fit our, smaller network\n",
    "    state_dict['classifier.6.weight'] = torch.randn(10, 4096)\n",
    "    state_dict['classifier.6.bias'] = torch.randn(10)\n",
    "\n",
    "    # Apply the state dict and set the classifer (layer part) to be the only thing we train\n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "    for param in model.features.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    model.optim = torch.optim.Adam(model.classifier.parameters())\n",
    "\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in_channels = next(iter(train_dataloader))[0].shape[1]\n",
    "# in_width_height = next(iter(train_dataloader))[0].shape[-1]\n",
    "# # Make a dummy model to find out the size before the first linear layer\n",
    "# CNN_model = VGG16S(num_classes=10, in_channels=in_channels)\n",
    "\n",
    "# # WARNING - THIS PART MIGHT BREAK\n",
    "# features_fore_linear = utils.get_dim_before_first_linear(CNN_model.features, in_width_height, in_channels, brain=True)\n",
    "\n",
    "# # Now make true model when we know how many features we have before the first linear layer\n",
    "# CNN_model = VGG16S(num_classes=10, in_channels=in_channels, features_fore_linear=7*7*512, dataset=test_set) \n",
    "\n",
    "# train_epochs = 5\n",
    "# train_accs, test_accs = CNN_model.train_model(train_dataloader, epochs=train_epochs,  val_dataloader=test_dataloader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in_channels = next(iter(train_dataloader))[0].shape[1]\n",
    "# in_width_height = next(iter(train_dataloader))[0].shape[-1]\n",
    "# Make a dummy model to find out the size before the first linear layer\n",
    "# CNN_model = VGG16D(num_classes=1000, in_channels=in_channels)\n",
    "\n",
    "# # WARNING - THIS PART MIGHT BREAK\n",
    "# features_fore_linear = utils.get_dim_before_first_linear(CNN_model.features, in_width_height, in_channels, brain=True)\n",
    "\n",
    "# Now make true model when we know how many features we have before the first linear layer\n",
    "CNN_model = VGG16D(num_classes=10, in_channels=3, features_fore_linear=512*7*7, dataset=test_set)\n",
    "# CNN_model = VGG16S(num_classes=10, in_channels=3, features_fore_linear=193600, dataset=test_set)\n",
    "\n",
    "train_epochs = 75\n",
    "train_accs, test_accs = CNN_model.train_model(train_dataloader, epochs=train_epochs,  val_dataloader=test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot train and test accuracies\n",
    "plt.plot(range(train_epochs), train_accs, label='Train')\n",
    "plt.plot(range(train_epochs), test_accs, label='Test')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Train and Test Accuracy over epochs, CNN model')\n",
    "plt.legend()\n",
    "now = datetime.now()\n",
    "filename = \"models/\" + slugify(f'{now.date()}_{now.hour}-{now.minute}-{now.second}')\n",
    "plt.savefig(filename+'.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_incorrect_classifications(model, dataloader):\n",
    "    \"\"\"\n",
    "    Run through the test dataloader and collect incorrectly classified images\n",
    "    and their respective true and predicted labels.\n",
    "    \"\"\"\n",
    "    model.eval()  # Ensure the model is in evaluation mode\n",
    "    \n",
    "    incorrect_images = []\n",
    "    incorrect_true_labels = []\n",
    "    incorrect_predicted_labels = []\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation for efficiency\n",
    "        for inputs, labels in tqdm(dataloader):\n",
    "            logits = model(inputs)\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            incorrect_idx = predictions != labels\n",
    "            \n",
    "            # Append only the incorrect classifications\n",
    "            incorrect_images.extend(inputs[incorrect_idx])\n",
    "            incorrect_true_labels.extend(labels[incorrect_idx])\n",
    "            incorrect_predicted_labels.extend(predictions[incorrect_idx])\n",
    "    \n",
    "    return incorrect_images, incorrect_true_labels, incorrect_predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect_images, incorrect_true_labels, incorrect_predicted_labels = get_incorrect_classifications(CNN_model, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(CNN_model, filename+'.pt')\n",
    "\n",
    "# Clean up to free memory\n",
    "torch.cuda.empty_cache()\n",
    "del train_set, validation_set, test_set, train_dataloader, validation_dataloader, test_dataloader, CNN_model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "for i in range(10):\n",
    "    # Calculate the row and column index for the grid\n",
    "    row, col = divmod(i, 5)\n",
    "    \n",
    "    # Access the corresponding image from incorrect_images and plot it\n",
    "    image = incorrect_images[i][0].cpu().detach().numpy()  # Convert to NumPy array\n",
    "    axes[row, col].imshow(image, cmap='viridis')  # You can change the colormap if needed\n",
    "    axes[row, col].axis('off')  # Turn off axis labels for a cleaner look\n",
    "    axes[row, col].set_title(f\"Image {i}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"Images/incorrect_images_{slugify(f'{now.date()}_{now.hour}-{now.minute}-{now.second}')}.pdf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
